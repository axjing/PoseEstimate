{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"coco-explorer.ipynb","provenance":[],"collapsed_sections":["D7Nn-niY7NE2"],"authorship_tag":"ABX9TyMl47fz23DgbBtZjeAA4EJ1"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WCMkvELD6F0a"},"source":["Run this to mount your personal google drive.\n","Ensure you have COCO-Human-Pose repo cloned into your drive at the path on line 6."]},{"cell_type":"code","metadata":{"id":"PR05EklpFe0k"},"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# Switch directories\n","%cd /content/gdrive/MyDrive/Colab\\ Data/COCO-Human-Pose\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ms51FNDh6na4"},"source":["Run this to load coco train/val keypoint annotations into an in-mem dataframe which can be queried. Ensure the paths make sense."]},{"cell_type":"code","metadata":{"id":"5PnqKa2lNeZ_"},"source":["# Run this block ONCE to load train and val annotations into mem and\n","# and create a useful df where each row is an annotation (person/crowd) instance\n","from pycocotools.coco import COCO\n","import numpy as np\n","import pandas as pd\n","import skimage.io as io\n","import matplotlib.pyplot as plt\n","\n","train_annot_path = 'COCO-Dataset/COCO-found-on-internet/2017/annotations/person_keypoints_train2017.json'\n","val_annot_path = 'COCO-Dataset/COCO-found-on-internet/2017/annotations/person_keypoints_val2017.json'\n","train_coco = COCO(train_annot_path) # load annotations for training set\n","val_coco = COCO(val_annot_path) # load annotations for validation set\n","\n","def get_meta(coco):\n","    ids = list(coco.imgs.keys())\n","    for i, img_id in enumerate(ids):\n","        img_meta = coco.imgs[img_id]\n","        ann_ids = coco.getAnnIds(imgIds=img_id)\n","        anns = coco.loadAnns(ann_ids)\n","        img_file_name = img_meta['file_name']\n","        w = img_meta['width']\n","        h = img_meta['height']\n","        \n","        yield [img_id, img_file_name, w, h, anns]\n","        \n","def convert_to_df(coco):\n","    images_data = []\n","    persons_data = []\n","    \n","    for img_id, img_fname, w, h, meta in get_meta(coco):\n","        images_data.append({\n","            'image_id': int(img_id),\n","            'src_set_image_id': int(img_id), # repeat id to reference after join\n","            'path': img_fname,\n","            'width': int(w),\n","            'height': int(h)\n","        })\n","        for m in meta: \n","            persons_data.append({\n","                'image_id': m['image_id'],\n","                'is_crowd': m['iscrowd'],\n","                'bbox': m['bbox'],\n","                'area': m['area'],\n","                'num_keypoints': m['num_keypoints'],            \n","                'keypoints': m['keypoints'],\n","                'segmentation': m['segmentation']\n","            })\n","    \n","    images_df = pd.DataFrame(images_data) \n","    images_df.set_index('image_id', inplace=True)\n","\n","    persons_df = pd.DataFrame(persons_data) \n","    persons_df.set_index('image_id', inplace=True)\n","    \n","    return images_df, persons_df\n","\n","images_df, persons_df = convert_to_df(train_coco)     \n","train_coco_df = pd.merge(images_df, persons_df, right_index=True, left_index=True)\n","train_coco_df['source'] = 0\n","train_coco_df.head()\n","\n","images_df, persons_df = convert_to_df(val_coco)      \n","val_coco_df = pd.merge(images_df, persons_df, right_index=True, left_index=True)\n","val_coco_df['source'] = 1\n","val_coco_df.head()\n","\n","coco_df = pd.concat([train_coco_df, val_coco_df], ignore_index=True)\n","# ^ Dataframe containing all val and test keypoint annotations"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tJQO-WWrzIug"},"source":["Run this to declare a function that displays Image w/ annotations by file name"]},{"cell_type":"code","metadata":{"id":"uIF0HJhXur9c"},"source":["# Filename is a globally unique identifier among all train/val/test splits\n","import skimage.io as io\n","import matplotlib.pyplot as plt\n","import pylab\n","pylab.rcParams['figure.figsize'] = (30.0, 30.0)\n","\n","def display_img(image_file_name):\n","  # Determine if img exists and if it is in train or val set\n","  img_df_rows = coco_df.loc[coco_df['path'] == image_file_name]\n","  if len(img_df_rows) == 0:\n","    print(\"Image with filename: \" + image_file_name + \" does not exist.\")\n","  else:\n","    coco = train_coco if img_df_rows['source'].iloc[0] == 0 else val_coco\n","\n","    # Get img id from file name\n","    imgId = img_df_rows['src_set_image_id'].iloc[0]\n","    img = coco.imgs[imgId]\n","    I = io.imread(img['coco_url']) # load image from URL (no need to store image locally)\n","\n","    # load and display keypoints annotations\n","    plt.subplot(1,2,1)\n","    plt.imshow(I)\n","    plt.axis('off')\n","\n","    plt.subplot(1,2,2)\n","    plt.imshow(I)\n","    annIds = coco.getAnnIds(imgIds=[imgId])\n","    anns = coco.loadAnns(annIds)\n","    coco.showAnns(anns)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dfNASRqc7Cfb"},"source":["Example of using the display_img function"]},{"cell_type":"code","metadata":{"id":"AqHJDitP7BSH"},"source":["image_file_name = '000000581667.jpg'\n","display_img(image_file_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cz2UysAqqvS0"},"source":["# see the contents of the coco-df\n","coco_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D7Nn-niY7NE2"},"source":["# All code blocks below are example queries on the coco df."]},{"cell_type":"code","metadata":{"id":"HOZ3LJyj8qPv"},"source":["# Number of images in train + val\n","len(coco_df.groupby('path',as_index=False).size())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xPBARmLJpZqT"},"source":["# Breakdown of num images with a given num annotations\n","coco_df.groupby('path',as_index=False).size().rename(columns = {'size': 'num_annotations'}).groupby('num_annotations',as_index=False).size().rename(columns={'size':'num_images'}).sort_values(by='num_images',ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ELObBd27yWl"},"source":["# images with crowds\n","coco_df.loc[coco_df['is_crowd'] == 1].groupby('path', as_index=False).size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ZoDvEkW9MEB"},"source":["# Breakdown of num of annotations with a given num keypoints\n","coco_df.groupby('num_keypoints',as_index=False).size().sort_values(by='size',ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b7oP0B3o9_Vm"},"source":["# Annotations with X keypoints\n","coco_df.loc[coco_df['num_keypoints'] == 0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_TWZQ7IDcLA"},"source":["# The image with the most people (20)\n","coco_df.groupby('path',as_index=False).size().loc[coco_df.groupby('path',as_index=False).size()['size'] == 20]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4YB1Qp9_GUzc"},"source":["# how many images have an annotation with x or more keypoints?\n","# how many of those images have only one annotation?\n","\n","# num images where best annotation has X keypoints\n","df = coco_df.groupby('path',as_index=False).max('num_keypoints').groupby('num_keypoints',as_index=False).size()\n","\n","# increase sizes so that sum represents all images with X or MORE keypoints\n","for i, row in df.iterrows():\n","  row['size'] += sum(df.loc[df['num_keypoints'] > row['num_keypoints']]['size'])\n","\n","df.rename(columns={'size':'num_imgs'},inplace=True)\n","\n","anns_per_img = coco_df.groupby('path',as_index=False).size()\n","imgs_w_one_ann = anns_per_img.loc[anns_per_img['size'] == 1]\n","imgs_w_one_ann = pd.merge(imgs_w_one_ann,coco_df)\n","df2 = imgs_w_one_ann.groupby('num_keypoints',as_index=False).size()\n","\n","# increase sizes so that sum represents all images with X or MORE keypoints\n","for i, row in df2.iterrows():\n","  row['size'] += sum(df2.loc[df['num_keypoints'] > row['num_keypoints']]['size'])\n","\n","df2.rename(columns={'size':'num_imgs_with_only_one_ann'},inplace=True)\n","\n","pd.merge(df,df2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7sCwWsvnooNz"},"source":["## major questions ##\n","# how will model handle images with more than one person?\n","# how will model handle images with half a person/person far away? (will num outputted keypoints be constant or dynamic?)"],"execution_count":null,"outputs":[]}]}