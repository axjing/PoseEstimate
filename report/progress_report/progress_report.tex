\documentclass[11pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[justification=centering]{caption}

% \hypersetup{
%     colorlinks,
%     linkcolor={red!50!black},
%     citecolor={blue!50!black},
%     urlcolor={blue!80!black}
% }

% geometry: for page margins
% graphicx: for inserting images
% hyperref: for adding hyperlinks to Figure references

% location for images
\graphicspath{ {./figures/} }

% \setlength{\parskip}{1em}

\title{\textbf{Human Pose Estimation: Progress Report}}
\author{Robert Lee, Julian Rocha, Wanze Zhang, Nicole Peverley, Rafay Chaudhry, Corey Koelewyn}
\date{\today}

\begin{document}

\maketitle

\section{The Problem}

Human pose estimation (HPE) is the problem domain of identifying body keypoints to construct a body model. Many existing systems accept images as input, with some implementations accepting data such as point cloud or videos. The application of HPE is widespread and benefits many industries. In particular, HPE can revolutionize industries such as augmented reality, animation, gaming, and robotics \cite{fritz_ai_hpe}. There are examples where a person’s gait can be used as an unique fingerprint for tracking or identifying individuals in videos \cite{ZENG_2012_92}. Another subset of HPE is hand pose estimation which can be used to translate sign language. HPE is a difficult problem domain due to challenges such as variability in human appearance and physique, environment lighting and weather, occlusions from other objects, self-occlusions from overlapping joints, complexity of movements of the human skeleton, and the inherent loss of information with a 2D image input \cite{Sigal2014}. This largely unsolved problem enables us to explore many novel and creative approaches, enriching our learning experience. We are excited to explore these applications, but we decided to limit our scope to a general version of the problem so we could reference the abundance of research available.

There are many variations of HPE systems, which can be roughly categorized into 2D vs 3D, single-person vs multi-person, and different body models. Our group has focused on single-frame monocular RGB images containing a single individual rather than a photo with multiple individuals. We believe this will be more feasible to complete. Given success with single individuals, we may explore multi-person HPE. Current state-of-the-art techniques for 2D single-person HPE can be categorized into two categories: regression on absolute joint position, or detection on joint locations with heat maps. Since a direct mapping from the input space to joint coordinates is a highly non-linear problem, heat-map-based approaches have proven to be more robust by including small-region information \cite{Chen_2020}. Thus, we have used the heat map approach.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{body_models.png}
    \caption{Common body models: (a) skeleton-based, (b) contour-based, (c) volume-based \cite{Chen_2020}}
    \label{fig:body_model}
\end{figure}

There are three different types of models used with full body HPE: kinematic, contour, and volumetric, as shown in Fig. \ref{fig:body_model} \cite{Chen_2020}. A kinematic model consists of points on each human joint connected by straight lines, similar to a stick-figure skeleton. The contour model consists of 2D squares and rectangles that represent the body, and the volumetric model represents the body with 3D cylinders. Further high-fidelity models implement meshes that capture more details of the human pose. The kinematic model is the simplest model to perform loss metric computations, and thus is preferred by our group as a scope-limiting decision. Using a basic kinematic model will simplify the problem space and encourage us to focus on tangible results that can be used in real life applications. Our goal is to estimate a kinematic model for the individual in each picture.

While there were many existing HPE datasets, very few perfectly matched our chosen requirements for the project. Any available datasets would need to be cleaned and processed. Many datasets contained assorted images that used different types of models for the pose estimation or consisted of multiple people in each image. Our primary choice is the COCO dataset \cite{coco_data}. This dataset consists of 330 K images, of which 200 K are labelled. There are pre-sorted subsets of this dataset specific for HPE competitions: COCO16 and COCO17. These contain 147 K images labelled with bounding boxes, joint locations, and human body masks. We decided to use COCO17 for our generator because. We have decided against using DensePose \cite{densepose}, which is a highly detailed manually annotated subset of the COCO dataset, because it does not offer joint coordinate labels. We still hope to use the MPII dataset \cite{mpii}, which consists of 41 K labelled images split into 29 K train and 12 K test, for validating our model’s performance if time permits. 


\section{Goals}
\label{SectionGoals}

The goal of this project is developing a HPE model that can perform with high accuracy and generalize well to unseen data. The success of the model will be measured using a subset of 2 quantifiable metrics common to the HPE literature \cite{Babu_2019}. 

The first metric that has been implemented for testing is Object Keypoint Similarity (OKS) as seen below in Figure \ref{fig:oks_evaluation_metrics}. OKS is commonly reported in the literature in terms of AR (average recall) and AP (average precision). It is implemented using the COCO keypoint evaluation server. COCO provides a Python API \cite{coco_keypoints} to evaluate results and compute precision and recall of OKS across scales. It requires that our model outputs be formatted according to the COCO keypoint detection standard \cite{coco_format_results}. Beyond model evaluation, the API also provides methods for detailed analysis of errors with plots that can be further explored.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{oks_evaluation_metrics.png}
    \caption{COCO Evaluation OKS Metrics}
    \label{fig:oks_evaluation_metrics}
\end{figure}

The second metric is Percentage of Correct Key Points (PCK) \cite{Cbsudux_2019} which will need to be implemented ourselves, separately from the COCO evaluation API. A detected joint is considered correct if the distance between the predicted and the true joint is within a certain threshold (threshold varies). PCKh@0.5 is when the threshold = 50\% of the head bone link. PCK.0.2 is when the distance between the predicted and true joint is less than 0.2x torso diameter.

After initial implementations of evaluation metrics and more research, we have decided to not pursue two metrics we mentioned in our formal proposal. The Percentage of Correct Parts (PCP) \cite{Cbsudux_2019} metric, where a detected joint is considered correct if the distance between the predicted joints and the true joint is at most half of the limb length, was determined to penalize shorter limbs and is not considered useful in practice. The other metric we chose not to include is Percentage of Detected Joints (PDJ) \cite{Cbsudux_2019}. For PDJ a detected joint is correct if the distance between the predicted and the true joint is within a certain fraction of the torso diameter. This metric is not as sensitive to the shorter limb problem since smaller people have shorter limbs and smaller torsos. However, this metric can penalize images that have people in different folded positions or torsos that aren’t visible.

The viability of this API has been explored and is allowing us to avoid implementing all of our own functions for computing evaluation metrics and align with existing industry-standard evaluation techniques. We will likely add to this evaluation by implementing our own metric for PCK.

For qualitative assessment of the model, a method to visualize the estimated keypoints and limbs overlayed on the input image has been implemented, see below in Figure \ref{fig:coco_evaluation_annotation}. This will be useful not only for visual demonstrations of the final product, but also to aid understanding the nature of any errors during the training/tuning phase. Should time permit, cross dataset evaluation will be used to demonstrate the model’s ability to generalize on a completely different data. Since different datasets have different labelling standards, some work will likely be required to format and prepare new datasets for evaluation. The COCO evaluation API will unfortunately not work with other dataset’s ground truth annotation data, however our implementation of PCK could still be used. The system should also perform with a reasonable latency.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth]{annotated_evaluation_image.png}
        \caption{Original COCO image \cite{coco_data}}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth]{original_evaluation_image.png}
        \caption{COCO image with result annotation}
    \end{subfigure}
    \caption{COCO Evaluation Annotation}
    \label{fig:coco_evaluation_annotation}
\end{figure}

\section{Plans and Progress To Date}

We completed consultations of research papers and online articles, and held weekly standup meetings. We completed the data generator and model training pipelines, enabling us to begin our first round of model training. The data generator has preliminary filtering and no augmentation, and the model training has variable architecture parameters and save/resume capability. We completed the majority of the Preparation milestone mentioned in our Proposal. The remaining task involves data augmentation, which was more complex than it appeared because the transformations need to be mirrored on the joint labels as well. A significant portion of the Architecture milestone is complete as well, with remaining work mostly in processing and evaluating the model output, and exploring techniques to improve training. A breakdown of our individual tasks, specific experiments and our initial results are outlined in \nameref{SectionInitialResults}. The specific details of future experiments are outlined in \nameref{SectionTaskB}. 

We have explored the feasibility of multi-person extension mentioned in the Proposal. The goal for this extension is to have the model perform and predict on images containing multiple humans. The theoretical process of achieving this is to have humans localized in the image, draw bounding boxes around them and then feed the image with multiple bounding boxes into the model. The model can then focus on each bounding box and treat it as a single-person input. A few different architectures, such as pre-trained HOG-based or CNN human detectors, have been explored. Please refer to \nameref{SectionInitialResults} for more details. Please also note that due to the limit of time and resources, multi-person extension is still in the experimental stage and may not be included in the final product.


\section{Task Breakdown}
\label{SectionTaskB}

All group members plan to work on hyperparameter tuning, which involves a number of parameters such as number of hourglass stacks, learning rate scheduler, loss function, batch size, feature depth at bottleneck, and data generator filtering options.
\subsection{Julian}
Julian is planning on performing experiments to see how adding data augmentation to the input pipeline will affect model performance. These experiments will include: varying bounding box size from 110\% to 150\%, flipping images horizontally, rotating images slightly, and various adjustments to brightness, contrast, and noise/grain. Augmentation will be done online between batch fetches and will replace examples. Due to the long training times of the current model, Julian may explore the option of converting a keras model to a TPU compatible model, which can be run on Google Colab TPU’s to hopefully reduce training time. Finally, Julian would like to explore hyperparameter tuning and training of the model.

\subsection{Robert}
Robert is planning to explore if there are better training metrics than accuracy and mean-squared error. These experiments will analyze whether the gradient of the loss value will approach zero too quickly and flatten out, which may cause the gradient to vanish. Robert will explore either data augmentation, visualization of intermediate layer heat maps, or TPU conversion mentioned above. He may also work on processing the model output to determine keypoints from the heatmap. 
\subsection{Wanze}
Wanze has been focusing on the multi-person extension mentioned above. However, since the extension might be out of scope of this project, Wanze will be switching to explore Data Augmentation with Julian and model visualizations. If time permits, Wanze would love to come back to work on  multi-person extension again - improve the accuracy of human localization within an image. 
\subsection{Rafay}
Rafay has worked on taking a set of test data provided by COCO and running evaluation metrics. So far the test data has been evaluated using the COCO API. The objective was to set up evaluation metrics that could be connected when necessary. Future steps will include adding metadata into the model’s output in order to run evaluation and implementing the Percentage of Correct Key Point metrics. Lastly, he will be focusing on tweaking the current loss functions or implementing a net new one.   
\subsection{Nicole}
Nicole is planning to work on implementing the Percentage of Correct Key Points metric that is not included in the COCO Evaluation API with Rafay. Another evaluation metric will be helpful to observe changes and we can possibly benefit from using this metric on other datasets if time permits. The model’s output format also needs to be updated to work with the previously tested COCO evaluation API for the OKS metric. We will need to add in some metadata fields to the data generator to determine confidence scores from the generated heatmaps. Lastly, she will focus on updating or changing the current loss function used in the model in order to get better results.
\subsection{Corey}
Corey is planning to work on t. 

\section{Initial Results}
\label{SectionInitialResults}

\subsection{Julian - Data Generator Pipeline}

Various metrics were gathered on the COCO dataset to help inform how the data should be processed as well as how the model should handle different scenarios. The two metrics most impactful to the project thus far are documented in Fig. \ref{fig:coco_metrics}. There are 66,808 images in the dataset containing a total of 273,469 annotations. Despite the fact that we have chosen to tackle single person and not multi person pose estimation, a large number of the COCO images contain more than one annotated person. It would be desirable if we did not have to discard these images, so cropping to a bounding box can allow us to convert a multi person image into a single person image. Most of the annotations in the dataset do not have all 17 key points of the body labelled. The model should not expect a perfect human image with all key points visible in frame. The model should instead output a dynamic number of keypoints based on what it can find. But what is the purpose of an annotation with 0 labelled keypoints? Fig. \ref{fig:zero_kp_images} shows two examples of these 0 keypoint annotations. Clearly the bounding boxes of these annotations denote people, but because there are no labeled keypoints, these examples may confuse our model. Therefore, despite the fact that 0 keypoint annotations make up 42.89\% of the total dataset annotations, these annotations will be filtered out during training.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth]{imgs_vs_anns.png}
        \caption{Number of images with a given number of annotations}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth]{anns_vs_kps.png}
        \caption{Number of annotations with a given number of keypoints}
    \end{subfigure}
    \caption{COCO metrics}
    \label{fig:coco_metrics}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth]{zero_kp_skatepark.png}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth]{zero_kp_elephant.png}
    \end{subfigure}
    \caption{Examples of annotations with zero labelled keypoints}
    \label{fig:zero_kp_images}
\end{figure}


The COCO dataset is more than 20GB so keeping the entire dataset in memory during training is not an option. Image preprocessing needs to be done to get the images and annotations in a format that can be passed to the model. A data generator was developed to tackle these two tasks. To prevent the generator from being the bottleneck of the training process, it makes use of multiple cores. The generator fetches images from disk in batches and the next batch can be fetched and processed while the model is performing the front and back propagation on the current batch. The preprocessing responsibilities of the data generator include: cropping to the ground truth bounding box of a person, resizing to the models input resolution and dimensions, and converting ground truth annotations for each keypoint to a heatmap for the cropped images. Fig. \ref{fig:data_gen_ex} shows an example of the transformations. Fig. \ref{fig:data_gen_ex} only shows the cropping for one person but since there are 4 annotated people, the image would get split into 4 images, each centered on the person of interest. Fig. \ref{fig:data_gen_ex} also only shows the heat map of the left hand, but since COCO annotations contain 17 keypoints, it produces 17 heatmaps per annotation. 

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=0.8\linewidth]{original_image.png}
        \caption{Original image}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=0.8\linewidth]{original_w_gt.png}
        \caption{Original image with ground truth annotations overlaid}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=0.8\linewidth]{processed_image.png}
        \caption{Processed image}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=0.8\linewidth]{processed_image_w_anns.png}
        \caption{Processed image with translated keypoints and left hand heat map overlaid}
    \end{subfigure}
    \caption{Example of transformations applied by the data generator}
    \label{fig:data_gen_ex}
\end{figure}


\subsection{Robert - Model Architecture and Training Pipeline}
Various model architecture types were evaluated on the following criteria: complexity, feasibility for this project timeline, and research paper results. The main resource was a survey paper outlining top 2D single-person pose estimation methods shown in Table \ref{tab:table_2d_hpe}. The majority of the top performing systems were based on an Hourglass backbone. Thus, we selected the network shown in Fig. \ref{fig:stacked_hourglass}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{stacked_hourglass}
    \caption{A stacked hourglass network for HPE \cite{newell2016stacked}}
    \label{fig:stacked_hourglass}
\end{figure}

The network architecture was implemented in Keras with a TensorFlow backend. It includes capability to adjust the number of hourglass modules, feature channels, input and output resolution, and the type of 2D convolution block (separable vs normal). Since this is an extremely deep network, we elected to use intermediate training to help gradients propagate into the earlier stages of the network. To verify model training, the Tensorboard logs for training sessions were analyzed. The training and validation graphs are shown in Figs. \ref{fig:hg_4_train} and \ref{fig:hg_4_val}, respectively, for a 4-stack network. The layer closest to the input side is numbered 0, and the output layer is numbered 3. We note that the trend of the loss, except for a blip in validation layer 3, has a downward trend, which is a good sign the model is learning something. We have not fully determined how the accuracy is assessed, so the absolute value of the loss is not analyzed. However, the accuracy seems to be improving with training. There is a small concern with the training loss flattening out and nearing 0 with values in the range of 1e-3, which may affect back propagation because of vanishing gradients. We hypothesize that mean-squared error may be inadequate because we use a heat map for each joint. Since the mean value is relatively consistent for our ground truth labels, a model guessing the mean may perform adequately for minimizing the loss, but perform horribly for the problem.

Google Colab training pipeline was finalized. We discovered that Colab had an issue accessing a mounted Google Drive folder containing more than 5,000 items. We first explored using symlinks that link on the Colab virtual machine (VM) disk to the mounted Google Drive shared folder. The files still needed to be visited once to add them to the cache. While this improved performance, we determined that it would be faster to download the dataset on the VM disk every time. Thus, we wrote a script that consistently sets up the Colab environment.

Finally, since Colab instances are time-limited, model saving and resuming was essential. This was implemented by command line arguments when the training script was run.


\subsection{Wanze - Multi-Person Detector}
A few architectures of human detection have been implemented and experimented. Here are a couple of examples with pros and cons indicated. Please refer to Fig. \ref{fig:multi_person_results} for visualizations.

\begin{itemize}
    \item \textbf{HOG-based}: 
    The advantages of using HOG function is the relatively high efficiency. Training time is significantly shorter than any other state-of-art architectures. However, the accuracy is not ideal. HOG uses colour gradient decent to identify human. It performs very well on images with clear human boundaries but very poorly if there’s any overlap 
involved.	
    \item \textbf{Pre-trained CNN \cite{pre_trained_architecture}}: 
    The advantages of using pre-trained solutions is that the accuracy significantly improves. As we can see the architecture correctly identifies all the human within this image. However, the inference time is a bit unbearable. It is safe to say that it would be too ambitious to predict on all multi-person images within the dataset with this architecture. 

\end{itemize}


All in all, there is a trade-off between efficiency and accuracy, which we have not yet found a healthy balance of. Given the limited time and resources, this extension may not be included in the final product.

\subsection{Rafay \& Nicole - Evaluation}
Please refer to \nameref{SectionGoals} for the results.

% TODO delete below, they’re comments here for reference on LaTeX style

% To add a figure, use the following:

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{FIGURE_NAME_HERE}
%     \caption{a nice plot}
%     \label{fig:fig1}
% \end{figure}

% To reference a figure, use the following:
% \ref{fig:fig1}

% To cite, use the following:
% \cite{NAME_HERE}

% Lorem ipsum dolor sit amet Fig. , consectetur adipiscing elit. Proin rhoncus mollis libero, non auctor risus feugiat ac. Quisque nec urna eu diam placerat gravida.\cite{Yan2013} Etiam rhoncus \cite{overleaf} $E=mc^2$.

% \begin{equation}
%     E=mc^2
% \end{equation}

\clearpage

\bibliographystyle{IEEEtran}
\bibliography{references}

\clearpage
\section{Appendix}
 
\begin{table}[h]
  \centering
  \caption{A summary of 2D single-person human pose estimation methods \cite{Chen_2020}}
  \includegraphics[width=1\textwidth]{summary_2d_1ppl_hpe}
  \label{tab:table_2d_hpe}
\end{table}


\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{train_layer_0_acc.png}
        \caption{Training Layer 0 Accuracy}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{train_layer_0_loss.png}
        \caption{Training Layer 0 Loss}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{train_layer_3_acc.png}
        \caption{Training Layer 3 Accuracy}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{train_layer_3_loss.png}
        \caption{Training Layer 3 Loss}
    \end{subfigure}
    \caption{Training accuracy and loss vs. epochs}
    \label{fig:hg_4_train}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{val_layer_0_acc.png}
        \caption{Validation Layer 0 Accuracy}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{val_layer_0_loss.png}
        \caption{Validation Layer 0 Loss}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{val_layer_3_acc.png}
        \caption{Validation Layer 3 Accuracy}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{val_layer_3_loss.png}
        \caption{Validation Layer 3 Loss}
    \end{subfigure}
    \caption{Validation accuracy and loss vs. epochs}
    \label{fig:hg_4_val}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{multi-person_HOG_bbox.png}
        \caption{multi-person bounding box with HOG}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{multi-person_pre-trained_bbox.png}
        \caption{multi-person bounding box with pre-trained architecture}
    \end{subfigure}
    \caption{Validation accuracy and loss vs. epochs}
    \label{fig:multi_person_results}
\end{figure}


\end{document}
